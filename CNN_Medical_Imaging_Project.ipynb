{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNu45Q7jTqQ3ep0JNXLvrK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatendatobaiwa/cnn/blob/main/CNN_Medical_Imaging_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SIMPLE CNN MODEL BUILD FOR LUNG CANCER IMAGING***\n"
      ],
      "metadata": {
        "id": "uTrMiFdV0cSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Mount Drive**"
      ],
      "metadata": {
        "id": "APUqAdq90e3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdecZ6Hd0iTL",
        "outputId": "110de706-6991-474b-f5ee-9f553cc9a37f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create Directories in Drive**"
      ],
      "metadata": {
        "id": "j1Feq2kl0v2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Install Dependenices**"
      ],
      "metadata": {
        "id": "WKEjmsby03rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.13.0\n",
        "!pip install -q tensorflow-addons==0.23.0\n",
        "!pip install albumentations==1.3.0\n",
        "!pip install numpy==1.24.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUdJXYJa04lM",
        "outputId": "d1678879-258c-48a0-cc5d-767279c3801f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.2/524.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic-core 2.27.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.61.1 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sqlalchemy 2.0.38 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "blosc2 3.1.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic 2.10.6 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.24.3 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.4.2 requires typing_extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.13.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.13.0 which is incompatible.\n",
            "langchain-core 0.3.37 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting albumentations==1.3.0\n",
            "  Downloading albumentations-1.3.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.24.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (6.0.2)\n",
            "Collecting qudida>=0.0.4 (from albumentations==1.3.0)\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2025.2.18)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.5.0)\n",
            "Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Installing collected packages: qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.4\n",
            "    Uninstalling albumentations-2.0.4:\n",
            "      Successfully uninstalled albumentations-2.0.4\n",
            "Successfully installed albumentations-1.3.0 qudida-0.0.4\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "project_path = '/content/drive/MyDrive/CNN_Medical_Imaging_Project'\n",
        "sys.path.append(project_path)"
      ],
      "metadata": {
        "id": "PqoGmx2I06o8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini Step 3a: Verify Fixes**"
      ],
      "metadata": {
        "id": "0yMbO_Y23psL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"TensorFlow Addons version: {tfa.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB0Nwn823qKa",
        "outputId": "51b82edf-bb76-4f10-cff4-6f18a6296df0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n",
            "TensorFlow Addons version: 0.23.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: data_processing.py: Load and Preprocess Data**"
      ],
      "metadata": {
        "id": "_eEmtj293vuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import albumentations as A\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class AlbumentationsSequence(Sequence):\n",
        "    \"\"\"Custom Sequence class for data augmentation with Albumentations.\"\"\"\n",
        "    def __init__(self, X, y, batch_size, transform):\n",
        "        self.X = X  # List or array of uint8 images\n",
        "        self.y = y  # Array of integer labels (or None for test data)\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_X = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_X_aug = [self.transform(image=x)['image'] for x in batch_X]\n",
        "        batch_X_aug = np.array(batch_X_aug)\n",
        "        batch_X_aug = np.stack((batch_X_aug,) * 3, axis=-1)  # Convert grayscale to 3-channel RGB\n",
        "        if self.y is not None:\n",
        "            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "            return batch_X_aug, batch_y\n",
        "        return batch_X_aug\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Class to handle loading and preprocessing of image data.\"\"\"\n",
        "    def __init__(self, target_size=(224, 224)):\n",
        "        self.target_size = target_size\n",
        "        self.class_names = ['benign', 'malignant', 'normal']\n",
        "\n",
        "    def load_images(self, data_dir):\n",
        "        \"\"\"Load images and labels from subdirectories (for training and validation).\"\"\"\n",
        "        data_dir = Path(data_dir)\n",
        "        images = []\n",
        "        labels = []\n",
        "        for label, class_name in enumerate(self.class_names):\n",
        "            class_dir = data_dir / class_name\n",
        "            if not class_dir.exists():\n",
        "                print(f\"Warning: Directory {class_dir} does not exist.\")\n",
        "                continue\n",
        "            img_paths = list(class_dir.glob('*'))\n",
        "            for img_path in img_paths:\n",
        "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "                    if img is not None:\n",
        "                        img = cv2.resize(img, self.target_size)\n",
        "                        images.append(img)\n",
        "                        labels.append(label)\n",
        "        if not images:\n",
        "            raise ValueError(f\"No valid images found in {data_dir}\")\n",
        "        images = np.array(images)\n",
        "        labels = np.array(labels, dtype=np.int32)\n",
        "        print(f\"Loaded {len(images)} images from {data_dir}\")\n",
        "        return images, labels, self.class_names\n",
        "\n",
        "    def load_test_images(self, test_dir):\n",
        "        \"\"\"Load test images from a flat directory (no labels).\"\"\"\n",
        "        test_dir = Path(test_dir)\n",
        "        images = []\n",
        "        img_paths = list(test_dir.glob('*'))\n",
        "        for img_path in img_paths:\n",
        "            if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, self.target_size)\n",
        "                    images.append(img)\n",
        "        if not images:\n",
        "            raise ValueError(f\"No valid images found in {test_dir}\")\n",
        "        images = np.array(images)\n",
        "        print(f\"Loaded {len(images)} test images from {test_dir}\")\n",
        "        return images\n",
        "\n",
        "    def create_generators(self, X_train, y_train, X_val, y_val, batch_size=32):\n",
        "        \"\"\"Create training and validation data generators with augmentations.\"\"\"\n",
        "        train_transform = A.Compose([\n",
        "            A.Rotate(limit=20, p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=0, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ElasticTransform(alpha=34, sigma=4, p=0.3),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            A.Normalize(mean=0, std=1),\n",
        "        ])\n",
        "        val_transform = A.Compose([\n",
        "            A.Normalize(mean=0, std=1),\n",
        "        ])\n",
        "        train_gen = AlbumentationsSequence(X_train, y_train, batch_size, train_transform)\n",
        "        val_gen = AlbumentationsSequence(X_val, y_val, batch_size, val_transform)\n",
        "        return train_gen, val_gen\n",
        "\n",
        "    def create_test_generator(self, X_test, batch_size=32):\n",
        "        \"\"\"Create a test data generator (no labels).\"\"\"\n",
        "        test_transform = A.Compose([\n",
        "            A.Normalize(mean=0, std=1),\n",
        "        ])\n",
        "        test_gen = AlbumentationsSequence(X_test, None, batch_size, test_transform)\n",
        "        return test_gen"
      ],
      "metadata": {
        "id": "OWzCeWnA32HU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: train.py: Model Training**"
      ],
      "metadata": {
        "id": "CtziedEK32e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from data_processing import DataProcessor\n",
        "from model_mobilenet import create_mobilenet_model\n",
        "\n",
        "def train_model(train_gen, val_gen, class_names, epochs=50):\n",
        "    \"\"\"Train and fine-tune the model with class weights and callbacks.\"\"\"\n",
        "    # Ensure train_gen.y is a NumPy array of integers\n",
        "    train_labels = np.array(train_gen.y, dtype=np.int32)\n",
        "    if len(train_labels) == 0:\n",
        "        raise ValueError(\"No training labels available in train_gen.\")\n",
        "\n",
        "    # Compute class weights to handle imbalance\n",
        "    classes = np.unique(train_labels)\n",
        "    if len(classes) == 0:\n",
        "        raise ValueError(\"No unique classes found in training labels.\")\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=train_labels)\n",
        "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weights_dict}\")\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = create_mobilenet_model(num_classes=len(class_names))\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Define callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "    # Initial training\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "        class_weight=class_weights_dict\n",
        "    )\n",
        "\n",
        "    # Fine-tuning: Unfreeze layers from block_13_expand\n",
        "    base_model = model.layers[1]  # MobileNetV2 base model layer\n",
        "    set_trainable = False\n",
        "    for layer in base_model.layers:\n",
        "        if layer.name == 'block_13_expand':\n",
        "            set_trainable = True\n",
        "        if set_trainable:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # Recompile with lower learning rate for fine-tuning\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Fine-tuning training\n",
        "    history_fine = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
        "        class_weight=class_weights_dict\n",
        "    )\n",
        "\n",
        "    return model, history, history_fine\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor = DataProcessor()\n",
        "\n",
        "    # Load and split training data\n",
        "    X, y, class_names = processor.load_images('/content/drive/MyDrive/CNN_Medical_Imaging_Project/data/raw/train')\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
        "    print(f\"Validation data: {X_val.shape}, {y_val.shape}\")\n",
        "\n",
        "    # Create generators\n",
        "    train_gen, val_gen = processor.create_generators(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Train the model\n",
        "    model, history, history_fine = train_model(train_gen, val_gen, class_names)\n",
        "\n",
        "    # Load and predict on test data\n",
        "    X_test = processor.load_test_images('/content/drive/MyDrive/CNN_Medical_Imaging_Project/data/raw/test')\n",
        "    test_gen = processor.create_test_generator(X_test)\n",
        "    if len(X_test) > 0:\n",
        "        y_pred = model.predict(test_gen)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        print(\"Predicted classes for test images:\", y_pred_classes)\n",
        "    else:\n",
        "        print(\"No test data loaded. Please check the test directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IXAqrXoA3_K8",
        "outputId": "94221d80-02d1-4092-a4a6-4a893acdf7b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 871 images from /content/drive/MyDrive/CNN_Medical_Imaging_Project/data/raw/train\n",
            "Training data: (696, 224, 224), (696,)\n",
            "Validation data: (175, 224, 224), (175,)\n",
            "Class weights: {0: 2.4166666666666665, 1: 0.8656716417910447, 2: 0.6987951807228916}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0583 - accuracy: 0.5273"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r22/22 [==============================] - 36s 1s/step - loss: 1.0583 - accuracy: 0.5273 - val_loss: 0.5914 - val_accuracy: 0.7257 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 39s 2s/step - loss: 0.6859 - accuracy: 0.6968 - val_loss: 0.4801 - val_accuracy: 0.7886 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.6051 - accuracy: 0.7443 - val_loss: 0.6399 - val_accuracy: 0.6171 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.6088 - accuracy: 0.7198 - val_loss: 0.3174 - val_accuracy: 0.8686 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 33s 2s/step - loss: 0.5344 - accuracy: 0.7328 - val_loss: 0.3520 - val_accuracy: 0.8286 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.5510 - accuracy: 0.7744 - val_loss: 0.2832 - val_accuracy: 0.9086 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.5283 - accuracy: 0.7342 - val_loss: 0.4171 - val_accuracy: 0.7771 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.5446 - accuracy: 0.7716 - val_loss: 0.6150 - val_accuracy: 0.6457 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.4647 - accuracy: 0.7816 - val_loss: 0.2624 - val_accuracy: 0.8971 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 30s 1s/step - loss: 0.5577 - accuracy: 0.7443 - val_loss: 0.3834 - val_accuracy: 0.7771 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.5208 - accuracy: 0.7687 - val_loss: 0.2644 - val_accuracy: 0.9029 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 33s 1s/step - loss: 0.4265 - accuracy: 0.8003 - val_loss: 0.2807 - val_accuracy: 0.8800 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.4379 - accuracy: 0.8247 - val_loss: 0.2469 - val_accuracy: 0.9486 - lr: 5.0000e-04\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 27s 1s/step - loss: 0.4337 - accuracy: 0.8089 - val_loss: 0.2394 - val_accuracy: 0.9429 - lr: 5.0000e-04\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.4142 - accuracy: 0.8362 - val_loss: 0.2788 - val_accuracy: 0.8971 - lr: 5.0000e-04\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.4241 - accuracy: 0.8003 - val_loss: 0.2597 - val_accuracy: 0.9086 - lr: 5.0000e-04\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.3985 - accuracy: 0.8305 - val_loss: 0.1789 - val_accuracy: 0.9200 - lr: 5.0000e-04\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.4293 - accuracy: 0.8161 - val_loss: 0.1869 - val_accuracy: 0.9371 - lr: 5.0000e-04\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 27s 1s/step - loss: 0.3986 - accuracy: 0.8276 - val_loss: 0.2551 - val_accuracy: 0.9086 - lr: 5.0000e-04\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.4396 - accuracy: 0.7859 - val_loss: 0.2567 - val_accuracy: 0.9143 - lr: 5.0000e-04\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.3955 - accuracy: 0.8420 - val_loss: 0.2196 - val_accuracy: 0.9314 - lr: 2.5000e-04\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.4077 - accuracy: 0.8391 - val_loss: 0.2426 - val_accuracy: 0.9257 - lr: 2.5000e-04\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.4155 - accuracy: 0.8276 - val_loss: 0.2076 - val_accuracy: 0.9371 - lr: 2.5000e-04\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.3943 - accuracy: 0.8333 - val_loss: 0.1861 - val_accuracy: 0.9314 - lr: 1.2500e-04\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.3820 - accuracy: 0.8649 - val_loss: 0.2153 - val_accuracy: 0.9314 - lr: 1.2500e-04\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.3618 - accuracy: 0.8506 - val_loss: 0.1857 - val_accuracy: 0.9371 - lr: 1.2500e-04\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 29s 1s/step - loss: 0.3748 - accuracy: 0.8578 - val_loss: 0.2103 - val_accuracy: 0.9371 - lr: 6.2500e-05\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 32s 1s/step - loss: 0.3825 - accuracy: 0.8434 - val_loss: 0.1911 - val_accuracy: 0.9371 - lr: 6.2500e-05\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.4010 - accuracy: 0.8477 - val_loss: 0.2087 - val_accuracy: 0.9371 - lr: 6.2500e-05\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 34s 2s/step - loss: 0.3529 - accuracy: 0.8621 - val_loss: 0.1933 - val_accuracy: 0.9371 - lr: 3.1250e-05\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.3498 - accuracy: 0.8750 - val_loss: 0.1892 - val_accuracy: 0.9371 - lr: 3.1250e-05\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 28s 1s/step - loss: 0.3858 - accuracy: 0.8477 - val_loss: 0.1969 - val_accuracy: 0.9371 - lr: 3.1250e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Conv2D' object has no attribute 'layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8c50141c8774>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_fine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Load and predict on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8c50141c8774>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_gen, val_gen, class_names, epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# MobileNetV2 base model layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'block_13_expand'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Conv2D' object has no attribute 'layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: evaluate.py: Model Evaluation**"
      ],
      "metadata": {
        "id": "kIfjvQec4FK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, class_names, save_path=None):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model on test data with classification metrics and visualizations.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained Keras model.\n",
        "    - X_test: Test images (numpy array).\n",
        "    - y_test: True labels (numpy array).\n",
        "    - class_names: List of class names (e.g., ['benign', 'malignant', 'normal']).\n",
        "    - save_path: Optional path to save confusion matrix plot (e.g., 'confusion_matrix.png').\n",
        "    \"\"\"\n",
        "    # Predict probabilities and classes\n",
        "    y_pred_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    report = classification_report(y_test, y_pred_classes, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Confusion matrix saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Per-Class Sensitivity and Specificity\n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        tp = cm[i, i]\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "        fp = np.sum(cm[:, i]) - tp\n",
        "        tn = np.sum(cm) - tp - fn - fp\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        print(f\"{class_name}: Sensitivity = {sensitivity:.2f}, Specificity = {specificity:.2f}\")\n",
        "\n",
        "    # ROC-AUC Score\n",
        "    y_test_bin = label_binarize(y_test, classes=range(len(class_names)))\n",
        "    auc = roc_auc_score(y_test_bin, y_pred_probs, multi_class='ovr')\n",
        "    print(f\"\\nROC-AUC Score (One-vs-Rest): {auc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from data_processing import DataProcessor\n",
        "\n",
        "    # Load test data instead of validation data\n",
        "    processor = DataProcessor()\n",
        "    X_test, y_test, class_names = processor.load_test_images('/content/drive/MyDrive/CNN_Medical_Imaging_Project/data/raw/test')\n",
        "\n",
        "    # Load the trained model\n",
        "    model = load_model('best_model.h5')\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model, X_test, y_test, class_names, save_path='confusion_matrix.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "pR37Q85d4HjO",
        "outputId": "44e2e767-be4a-46d1-bac3-cbf3869084c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7082aa119d62>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'confusion_matrix.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-7082aa119d62>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test, class_names, save_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Predict probabilities and classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0my_pred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0my_pred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2577\u001b[0m                         )\n\u001b[1;32m   2578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2580\u001b[0m                     \u001b[0;34m\"Unexpected result of `predict_function` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m                     \u001b[0;34m\"(Empty batch_outputs). Please use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: xai.py: Implementation of Explainable AI (Grad-CAM)**"
      ],
      "metadata": {
        "id": "UCs2RgYL4M7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from data_processing import DataProcessor\n",
        "\n",
        "def get_gradcam_heatmap(model, img_array, layer_name, class_index):\n",
        "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, class_index]\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def superimpose_heatmap(img, heatmap, alpha=0.4):\n",
        "    \"\"\"Superimpose the heatmap on the original image.\"\"\"\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    img_rgb = np.stack((img,) * 3, axis=-1) * 255  # Convert grayscale to RGB\n",
        "    superimposed_img = heatmap * alpha + img_rgb\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "    return superimposed_img\n",
        "\n",
        "def visualize_explanations(model, X, layer_name, num_images=5):\n",
        "    test_gen = processor.create_test_generator(X)\n",
        "    for i in range(min(num_images, len(X))):\n",
        "        img = test_gen[i // test_gen.batch_size][i % test_gen.batch_size:i % test_gen.batch_size + 1]\n",
        "        pred_probs = model.predict(img)\n",
        "        pred_class = np.argmax(pred_probs)\n",
        "        heatmap = get_gradcam_heatmap(model, img, layer_name, pred_class)\n",
        "        superimposed_img = superimpose_heatmap(X[i], heatmap)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(X[i], cmap='gray')\n",
        "        plt.title(f'Predicted: {class_names[pred_class]}')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.title('Grad-CAM')\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor = DataProcessor()\n",
        "    X_test = processor.load_test_images('/content/drive/MyDrive/CNN_Medical_Imaging_Project/data/raw/test')\n",
        "    model = load_model('best_model.h5')\n",
        "    visualize_explanations(model, X_test, 'block_13_expand')"
      ],
      "metadata": {
        "id": "dX1c5wcn4TBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}